{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract a network of pages from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example we query MediaWiki's [API](https://www.mediawiki.org/wiki/API:Main_page) to build a network starting from a list of \"seed\" pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the wikipedia edition we will work on {lang}.wikipedia.org\n",
    "lang = 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our list of seed articles is contained in `right_to_be_forgotten_seed.txt` in the `seeds` directory. There are other seed files in the `seeds` folder if you want to look them up. Now, let's take a look at `right_to_be_forgotten_seed.txt` with `cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right_to_be_forgotten\r\n"
     ]
    }
   ],
   "source": [
    "# we temporaily cd to the seeds dir and then print the contents of right_to_be_forgotten_seed.txt \n",
    "! ( cd 'seeds' && cat right_to_be_forgotten_seed.txt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "# the seed file is ./right_to_be_forgotten_seed.txt\n",
    "seed_file = pathlib.Path('seeds/right_to_be_forgotten_seed.txt')\n",
    "\n",
    "# get the seed file without extension\n",
    "seed_filename_noext = os.path.splitext(seed_file.name)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the directory structure where we will save the results from the API, it will look like this:\n",
    "```\n",
    "data\n",
    "├── links\n",
    "│   ├── articles.txt\n",
    "│   └── ...\n",
    "└── results\n",
    "    ├── extract_network.log\n",
    "    ├── <seed_name>.csv\n",
    "    └── network_<seed_name>.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing the seed articles (./data)\n",
    "data_folder = pathlib.Path('data')\n",
    "# directory containing the outlinks of each article (./data/links)\n",
    "link_folder = data_folder/'links'\n",
    "# directory containing the resulting networks (./data/results)\n",
    "results_folder = data_folder/'results'\n",
    "\n",
    "link_folder.mkdir(parents=True, exist_ok=True)\n",
    "results_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# setup the logging to file ./data/results/extract_network.log\n",
    "log_name = 'extract_network.log'\n",
    "log_filename = (results_folder/log_name)\n",
    "\n",
    "logging.basicConfig(filename=log_filename, level=logging.DEBUG)\n",
    "\n",
    "logging.debug('This message should go to the log file')\n",
    "logging.info('So should this')\n",
    "logging.warning('And this, too')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file containing the network ./data/results/<seed>.csv\n",
    "net_filename = 'network_' + seed_filename_noext + '.csv'\n",
    "net_file = (results_folder/net_filename).open('w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read our seed file in a dictionary, ignoring empty lines and comments (lines starting with #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seed(file_name):\n",
    "    \"\"\"\n",
    "    Load the seed file, ignoring empty lines and comments (lines starting with #)\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    infile = pathlib.Path(file_name).open('r')\n",
    "    for line in infile:\n",
    "        el = line.strip('\\n')\n",
    "        if el and el[0] != '#':\n",
    "            el = el.replace(' ', '_')\n",
    "            dic[el] = 1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Right_to_be_forgotten': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_seed(seed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikitools import wiki\n",
    "from wikitools import api\n",
    "\n",
    "# create a Wiki object\n",
    "site = wiki.Wiki(\"https://{lang}.wikipedia.org/w/api.php\".format(lang=lang))\n",
    "\n",
    "# get outlinks of a wikipedia article through wiki api\n",
    "def get_outlinks_from_api(title):\n",
    "    p_id = -1\n",
    "    outlinks = []\n",
    "\n",
    "    if title == '' or title == ' ':\n",
    "        return p_id, outlinks\n",
    "\n",
    "    params = {'action': 'query',\n",
    "              'prop': 'revisions', \n",
    "              'titles': title,\n",
    "              'rvprop':'content',\n",
    "              'redirects':1\n",
    "              }\n",
    "    request = api.APIRequest(site, params)\n",
    "\n",
    "    logging.debug('query: {}'.format(params))\n",
    "\n",
    "    result = request.query()\n",
    "    if int(list(result['query']['pages'].keys())[0]) < 1:\n",
    "        logging.warning('ARTICLE NOT FOUND: {}'.format(title))\n",
    "        return (p_id, outlinks)\n",
    "\n",
    "    else:\n",
    "        outlinks = []\n",
    "\n",
    "        p_id = list(result['query']['pages'].keys())[0]\n",
    "        rev = result['query']['pages'][p_id]['revisions'][0]\n",
    "        content = rev['*']\n",
    "\n",
    "        links = parse_text(content)\n",
    "        for l in links:\n",
    "            target = l.replace(' ', '_')\n",
    "            outlinks.append(target)\n",
    "\n",
    "        return (p_id, outlinks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlinks(title):\n",
    "    outlinks_filename = '{}_articles.txt'.format(title.replace('/','.'))\n",
    "    outlinks_saved = link_folder/outlinks_filename\n",
    "\n",
    "    # data saved in the disc\n",
    "    is_new_page = False\n",
    "    try:\n",
    "        with outlinks_saved.open('r') as f:\n",
    "            outlinks_checked = f.read().splitlines()\n",
    "    except IOError as e:\n",
    "        outlinks_checked = []\n",
    "        \n",
    "    if outlinks_checked:\n",
    "        logging.info('{} saved links from: {}'.format(len(outlinks_checked), title))\n",
    "    else:\n",
    "        # get data through wiki API\n",
    "        is_new_page = True\n",
    "\n",
    "        # p is the page_id\n",
    "        (p, outlinks) = get_outlinks_from_api(title)\n",
    "        \n",
    "        (redirects, outlinks_checked) = check_redirects(outlinks)\n",
    "\n",
    "    return (is_new_page, outlinks_checked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_param_list(alist):\n",
    "    chunk_len = 50\n",
    "    if len(alist) < 1:\n",
    "        return ''\n",
    "    s = ['']\n",
    "    l = 0\n",
    "    i = 0\n",
    "\n",
    "    chunks = [alist[x:x+chunk_len]\n",
    "              for x in range(0, len(alist), chunk_len)\n",
    "              ]\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check redirect in Wikipedia articles/links\n",
    "def check_redirects(titles):\n",
    "    redirects = {}\n",
    "    links = {}\n",
    "    duplicates = 0\n",
    "\n",
    "    title_lists = split_param_list(titles)\n",
    "    for title_list in title_lists:\n",
    "\n",
    "        params = {'action':'query',\n",
    "                  'titles': '|'.join(title_list).strip('|'),\n",
    "                  'redirects': 1\n",
    "                  }\n",
    "        request = api.APIRequest(site, params)\n",
    "\n",
    "    \n",
    "        logging.debug('query: {}'.format(params))\n",
    "\n",
    "        result = request.query()\n",
    "        logging.debug('result: {}'.format(result))\n",
    "\n",
    "        if 'redirects' in result['query']:\n",
    "            for redir in result['query']['redirects']:\n",
    "                redirects[redir['from']] = redir['to']\n",
    "\n",
    "        for page in result['query']['pages']:\n",
    "            if page != '-1' and 'ns' in result['query']['pages'][page]:\n",
    "                if result['query']['pages'][page]['ns'] == 0:\n",
    "                    link = result['query']['pages'][page]['title'].replace(' ', '_')\n",
    "                    if link in links:\n",
    "                        duplicates += 1\n",
    "                    links[link] = page\n",
    "\n",
    "    missing = len(titles) - (len(links) + duplicates)\n",
    "    if missing != 0:\n",
    "        logging.debug('{} missing redirects '\n",
    "                      '({} titles,  {} found, {} duplicates)'\n",
    "                      .format(missing, len(titles), len(links), duplicates)\n",
    "                      )\n",
    "    return redirects,links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regular expressions to find URLs in text\n",
    "linkSimpleP = re.compile(r'\\[\\[(.+?)[][|{}/#]')\n",
    "linkGreedyP = re.compile(r'\\[\\[([^]^[^}^{^#^/^|]+)')\n",
    "linkP = re.compile(r'\\[\\[([^]^[^}^{^#^/^]+?)\\s*(?:/[^]^[]*?)?\\s*(?:\\|[^]^[]*?)?(?:\\}\\})?\\s*\\]\\]')\n",
    "\n",
    "def parse_text(content):\n",
    "    links = {}\n",
    "    rough_links = re.findall(linkP, content) #get all links\n",
    "\n",
    "    title_lists = split_param_list(rough_links)\n",
    "\n",
    "    for title_list in title_lists:\n",
    "        params = {'action': 'query',\n",
    "                  'titles': '|'.join(title_list).strip('|'),\n",
    "                  'format': 'json',\n",
    "                  'redirect': 1\n",
    "                  }\n",
    "        request = api.APIRequest(site, params)\n",
    "        \n",
    "        logging.info('query: {}'.format(params))\n",
    "    \n",
    "        result = request.query()\n",
    "\n",
    "        logging.debug('result: {}'.format(result))\n",
    "\n",
    "        # check that we have received pages as result\n",
    "        if result['query'].get('pages', None):\n",
    "            for page in result['query']['pages']:\n",
    "                if page != '-1' and ('ns' in result['query']['pages'][page]):\n",
    "                    # Filter just useful links, articles are in ns == 0\n",
    "                    if result['query']['pages'][page]['ns'] == 0:\n",
    "                        link = result['query']['pages'][page]['title']\n",
    "                        links[link] = 1\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import csv\n",
    "\n",
    "csv_fields = [\n",
    "'seed', \n",
    "'links_from_seed',\n",
    "'links_to_seed',\n",
    "'in_degree',\n",
    "'out_degree',\n",
    "'out_WP'\n",
    "]\n",
    "\n",
    "def extract_network(seed_file):\n",
    "\n",
    "    # list containing all outlinks of seed pages\n",
    "    links_to_seedlinks = []\n",
    "\n",
    "    # read the list of seed defined by the user\n",
    "    seeds_list = load_seed(seed_file)\n",
    "\n",
    "    first_step = {}\n",
    "\n",
    "    for seed in set(seeds_list.keys()):\n",
    "        print(\"--> new seed article: {}\\n\".format(seed))\n",
    "\n",
    "        (is_new_page, outlinks_checked) = get_outlinks(seed) # get outlink for current seed\n",
    "        links_to_seedlinks.append(outlinks_checked) # append this outlinks to the list of all seeds' outlinks\n",
    "\n",
    "        if is_new_page: ## data not already saved in disk, save it\n",
    "            page_list_filename = seed.replace('/','.') + '_articles.txt'\n",
    "            page_list_file = (link_folder/page_list_filename).open('a+')\n",
    "\n",
    "            ##########\n",
    "            # update degrees for seed and current target outlink\n",
    "            ##########\n",
    "            if seed in first_step:\n",
    "                first_step[seed]['out_WP'] = len(outlinks_checked)\n",
    "\n",
    "        for target in outlinks_checked: # outlinks_checked contains the outlinks of the current keyword\n",
    "            if seed != target:\n",
    "                # write edge as pair of nodes (source, target)\n",
    "                net_file.write(seed + '\\t' + target  + '\\n')\n",
    "                if is_new_page:\n",
    "                    page_list_file.write(target + '\\n')\n",
    "                try:\n",
    "                    first_step[seed]['out_degree'] += 1\n",
    "                    if(target in seeds_list):\n",
    "                        first_step[seed]['links_to_seed'] += 1\n",
    "                except:\n",
    "                    if target in seeds_list:\n",
    "                        first_step[seed] = {'seed': True,\n",
    "                                            'links_from_seed': 0,\n",
    "                                            'links_to_seed': 1,\n",
    "                                            'in_degree': 0,\n",
    "                                            'out_degree': 1,\n",
    "                                            'out_WP': len(outlinks_checked)\n",
    "                                            }\n",
    "                    else:\n",
    "                        first_step[seed] = {'seed': True,\n",
    "                                            'links_from_seed': 0,\n",
    "                                            'links_to_seed': 0,\n",
    "                                            'in_degree': 0,\n",
    "                                            'out_degree': 1,\n",
    "                                            'out_WP': len(outlinks_checked)\n",
    "                                            }\n",
    "                try:\n",
    "                    first_step[target]['links_from_seed'] += 1\n",
    "                    first_step[target]['in_degree'] += 1\n",
    "                    if(target in seeds_list):\n",
    "                        first_step[target]['seed'] = True\n",
    "                except:\n",
    "                    if target in seeds_list:\n",
    "                        first_step[target] = {'seed': True,\n",
    "                                              'links_from_seed': 1,\n",
    "                                              'links_to_seed': 0,\n",
    "                                              'in_degree': 1,\n",
    "                                              'out_degree': 0,\n",
    "                                              'out_WP': 0\n",
    "                                              }\n",
    "                    else:\n",
    "                        first_step[target] = {'seed': False,\n",
    "                                              'links_from_seed': 1,\n",
    "                                              'links_to_seed': 0,\n",
    "                                              'in_degree': 1,\n",
    "                                              'out_degree': 0,\n",
    "                                              'out_WP': 0\n",
    "                                              }\n",
    "\n",
    "        if is_new_page:\n",
    "            page_list_file.close()\n",
    "\n",
    "    # from the list of outlinks keep only those that are no seeds and remove duplicates\n",
    "    links_to_seedlinks = list(itertools.chain.from_iterable(links_to_seedlinks))\n",
    "    links_to_seedlinks = list(set(links_to_seedlinks) - set(seeds_list.keys()) )\n",
    "\n",
    "    ##########\n",
    "    # Start new iteraction over outlinks of the seed articles\n",
    "    ##########\n",
    "    olink_index = 1 # just a counter\n",
    "    for title in links_to_seedlinks:\n",
    "        print(\"  -> outlink {} ({} of {})\"\n",
    "              .format(title, olink_index, len(links_to_seedlinks))\n",
    "             )\n",
    "\n",
    "        olink_index += 1\n",
    "        (is_new_page, outlinks_checked) = get_outlinks(title) # get outlinks of current articles\n",
    "\n",
    "        if is_new_page: ## data not already saved in disk, save it\n",
    "            page_list_file_title_name = title.replace('/','.') + '_articles.txt'\n",
    "            page_list_file_title = (link_folder/page_list_file_title_name).open('w')\n",
    "\n",
    "            ##########\n",
    "            # update degress of source and target\n",
    "            ##########\n",
    "            first_step[title]['out_WP'] = len(outlinks_checked)\n",
    "        for target in outlinks_checked: # outlinks_checked contains the outlinks of the current keyword\n",
    "            if is_new_page:\n",
    "                page_list_file_title.write(target + '\\n')\n",
    "            if target in seeds_list.keys( ) and target != title:\n",
    "                net_file.write( title + '\\t' + target  + '\\n') # write edge as pair of nodes\n",
    "                first_step[title]['links_to_seed'] += 1\n",
    "                first_step[title]['out_degree'] += 1\n",
    "                first_step[target]['in_degree'] += 1\n",
    "            else:\n",
    "                if target in links_to_seedlinks and target != title:\n",
    "                    net_file.write( title + '\\t' + target  + '\\n') # write edge as pair of nodes\n",
    "                    first_step[title]['out_degree'] += 1\n",
    "                    first_step[target]['in_degree'] += 1\n",
    "        # note that I write inside each if because I write only if the target is either a seed or a outlink of a seed (since we\n",
    "        # only want the first step degree net)\n",
    "        ##########\n",
    "        ##########\n",
    "        if is_new_page:\n",
    "            page_list_file_title.close()\n",
    "\n",
    "    net_file.close()\n",
    "    # Write degrees in a file\n",
    "    output_filename = 'first_step_degrees_{}.csv'.format(seed_filename_noext)\n",
    "    with (results_folder/output_filename).open('w') as outfile:\n",
    "        writer = csv.writer(outfile, delimiter =\"\\t\")\n",
    "        writer.writerow(['Page'] + csv_fields)\n",
    "        for page in first_step.keys():\n",
    "            writer.writerow([page] + [first_step[page][degree] for degree in csv_fields])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links  results\r\n"
     ]
    }
   ],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> new seed article: Right_to_be_forgotten\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristian/.linuxbrew/opt/python/lib/python3.7/site-packages/wikitools/api.py:108: FutureWarning: The querycontinue option is deprecated and will be removed\n",
      "in a future release, use the new queryGen function instead\n",
      "for queries requring multiple requests\n",
      "  for queries requring multiple requests\"\"\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> outlink Right_to_privacy (1 of 106)\n",
      "  -> outlink Cease_and_desist (2 of 106)\n",
      "  -> outlink Twitter (3 of 106)\n",
      "  -> outlink Child_prodigy (4 of 106)\n",
      "  -> outlink Electronic_Frontier_Foundation (5 of 106)\n",
      "  -> outlink The_New_Yorker (6 of 106)\n",
      "  -> outlink Conservative_Party_(UK) (7 of 106)\n",
      "  -> outlink Deutschlandradio (8 of 106)\n",
      "  -> outlink Rehabilitation_of_Offenders_Act_1974 (9 of 106)\n",
      "  -> outlink Historical_negationism (10 of 106)\n",
      "  -> outlink Hamburg (11 of 106)\n",
      "  -> outlink Max_Schrems (12 of 106)\n",
      "  -> outlink Martin_v._Hearst_Corporation (13 of 106)\n",
      "  -> outlink Federal_Constitutional_Court (14 of 106)\n",
      "  -> outlink Commission_nationale_de_l'informatique_et_des_libertés (15 of 106)\n",
      "  -> outlink Google_Groups (16 of 106)\n",
      "  -> outlink Accountability (17 of 106)\n",
      "  -> outlink Walter_Sedlmayr (18 of 106)\n",
      "  -> outlink Larry_Page (19 of 106)\n",
      "  -> outlink William_James_Sidis (20 of 106)\n",
      "  -> outlink Consumer_Watchdog (21 of 106)\n",
      "  -> outlink Audiencia_Nacional (22 of 106)\n",
      "  -> outlink Dato_Capital (23 of 106)\n",
      "  -> outlink Jennifer_Granick (24 of 106)\n",
      "  -> outlink European_Commission (25 of 106)\n",
      "  -> outlink Jimmy_Wales (26 of 106)\n",
      "  -> outlink Federal_Court_of_Justice (27 of 106)\n",
      "  -> outlink Tony_Avella (28 of 106)\n",
      "  -> outlink Right_to_disconnect (29 of 106)\n",
      "  -> outlink Stanley_O'Neal (30 of 106)\n",
      "  -> outlink Business_intelligence (31 of 106)\n",
      "  -> outlink Web_search_engine (32 of 106)\n",
      "  -> outlink United_States_Constitution (33 of 106)\n",
      "  -> outlink Wired_(magazine) (34 of 106)\n",
      "  -> outlink Google (35 of 106)\n",
      "  -> outlink David_Weprin (36 of 106)\n",
      "  -> outlink The_Atlantic (37 of 106)\n",
      "  -> outlink Korea_Communications_Commission (38 of 106)\n",
      "  -> outlink Streisand_effect (39 of 106)\n",
      "  -> outlink Spanish_Data_Protection_Agency (40 of 106)\n",
      "  -> outlink Freedom_of_speech (41 of 106)\n",
      "  -> outlink Freedom_of_the_press_in_the_United_States (42 of 106)\n",
      "  -> outlink Duke_Law_Journal (43 of 106)\n",
      "  -> outlink Google_Spain_v_AEPD_and_Mario_Costeja_González (44 of 106)\n",
      "  -> outlink University_of_Cambridge (45 of 106)\n",
      "  -> outlink Karlsruhe (46 of 106)\n",
      "  -> outlink Viviane_Reding (47 of 106)\n",
      "  -> outlink Integrity (48 of 106)\n",
      "  -> outlink European_Union (49 of 106)\n",
      "  -> outlink New_York_(state) (50 of 106)\n",
      "  -> outlink Yahoo! (51 of 106)\n",
      "  -> outlink The_Daily_Telegraph (52 of 106)\n",
      "  -> outlink Fortune_(magazine) (53 of 106)\n",
      "  -> outlink Virginia_da_Cunha (54 of 106)\n",
      "  -> outlink Argentina (55 of 106)\n",
      "  -> outlink Human_rights (56 of 106)\n",
      "  -> outlink Digital_privacy (57 of 106)\n",
      "  -> outlink International_Safe_Harbor_Privacy_Principles (58 of 106)\n",
      "  -> outlink Yale_Law_Journal (59 of 106)\n",
      "  -> outlink Tiziana_Cantone (60 of 106)\n",
      "  -> outlink YouTube (61 of 106)\n",
      "  -> outlink The_Red_Kimono (62 of 106)\n",
      "  -> outlink Jurisdiction (63 of 106)\n",
      "  -> outlink Gerry_Hutch (64 of 106)\n",
      "  -> outlink United_Kingdom_general_election,_2017 (65 of 106)\n",
      "  -> outlink Censorship (66 of 106)\n",
      "  -> outlink Süddeutsche_Zeitung (67 of 106)\n",
      "  -> outlink Data_Protection_Directive (68 of 106)\n",
      "  -> outlink Wolfgang_Werlé_and_Manfred_Lauber (69 of 106)\n",
      "  -> outlink Dejan_Lazić (70 of 106)\n",
      "  -> outlink First_Amendment_to_the_United_States_Constitution (71 of 106)\n",
      "  -> outlink EU–US_Privacy_Shield (72 of 106)\n",
      "  -> outlink English_Wikipedia (73 of 106)\n",
      "  -> outlink Internet (74 of 106)\n",
      "  -> outlink Westphalian_sovereignty (75 of 106)\n",
      "  -> outlink Revenge_porn (76 of 106)\n",
      "  -> outlink Open_Society_Foundations (77 of 106)\n",
      "  -> outlink European_Court_of_Justice (78 of 106)\n",
      "  -> outlink The_Register (79 of 106)\n",
      "  -> outlink Celebrity (80 of 106)\n",
      "  -> outlink Internet_privacy (81 of 106)\n",
      "  -> outlink The_Washington_Post (82 of 106)\n",
      "  -> outlink Due_diligence (83 of 106)\n",
      "  -> outlink Index_on_Censorship (84 of 106)\n",
      "  -> outlink International_human_rights_law (85 of 106)\n",
      "  -> outlink Memory_hole (86 of 106)\n",
      "  -> outlink Article_29_Data_Protection_Working_Party (87 of 106)\n",
      "  -> outlink Baidu (88 of 106)\n",
      "  -> outlink Wikipedia (89 of 106)\n",
      "  -> outlink Intelligence_Squared (90 of 106)\n",
      "  -> outlink Information_privacy (91 of 106)\n",
      "  -> outlink Financial_Times (92 of 106)\n",
      "  -> outlink General_Data_Protection_Regulation (93 of 106)\n",
      "  -> outlink European_Parliament (94 of 106)\n",
      "  -> outlink Stanford_Law_Review (95 of 106)\n",
      "  -> outlink Theresa_May (96 of 106)\n",
      "  -> outlink Know_your_customer (97 of 106)\n",
      "  -> outlink Freedom_of_speech_in_the_United_States (98 of 106)\n",
      "  -> outlink Delhi_High_Court (99 of 106)\n",
      "  -> outlink Robert_Peston (100 of 106)\n",
      "  -> outlink Karnataka_High_Court (101 of 106)\n",
      "  -> outlink Social_stigma (102 of 106)\n",
      "  -> outlink Fundamental_rights (103 of 106)\n",
      "  -> outlink The_Guardian (104 of 106)\n",
      "  -> outlink The_New_York_Times (105 of 106)\n",
      "  -> outlink Wikimedia_Foundation (106 of 106)\n"
     ]
    }
   ],
   "source": [
    "extract_network(seed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is contained in `./data/results/network_right_to_be_forgotten_seed.csv`. Wa can visualize the first ten rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right_to_be_forgotten\tArgentina\r\n",
      "Right_to_be_forgotten\tBaidu\r\n",
      "Right_to_be_forgotten\tCease_and_desist\r\n",
      "Right_to_be_forgotten\tCensorship\r\n",
      "Right_to_be_forgotten\tChild_prodigy\r\n",
      "Right_to_be_forgotten\tConsumer_Watchdog\r\n",
      "Right_to_be_forgotten\tData_Protection_Directive\r\n",
      "Right_to_be_forgotten\tDavid_Weprin\r\n",
      "Right_to_be_forgotten\tDelhi_High_Court\r\n",
      "Right_to_be_forgotten\tDeutschlandradio\r\n"
     ]
    }
   ],
   "source": [
    "! head ./data/results/network_right_to_be_forgotten_seed.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see a visualization of a bigger network visit [this page](https://ngi4eu.github.io/engineroom-data-sprint-notebooks/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
